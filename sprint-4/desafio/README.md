# Desafio de An√°lise de Dados e Integra√ß√£o com AWS S3

Este projeto foi desenvolvido como parte de um desafio para um programa de bolsas. O objetivo √© demonstrar um fluxo de trabalho completo de an√°lise de dados, desde a limpeza at√© o upload seguro dos resultados para a nuvem da AWS.

## üìù Descri√ß√£o do Projeto

O projeto utiliza um conjunto de dados sobre movimentos migrat√≥rios no Brasil e √© dividido em duas etapas principais, representadas por dois Jupyter Notebooks:

1.  **`etl.ipynb` (Extra√ß√£o, Transforma√ß√£o e Limpeza):** Respons√°vel por carregar os dados brutos, realizar a limpeza e prepar√°-los para a an√°lise, salvando uma vers√£o processada.
2.  **`analise.ipynb` (An√°lise e Upload para AWS):** Carrega os dados limpos, realiza as an√°lises principais e, como passo final, faz o upload dos relat√≥rios gerados para um bucket no Amazon S3.

### An√°lises Realizadas
O notebook de an√°lise busca responder √†s seguintes perguntas-chave sobre o fluxo migrat√≥rio:

* **An√°lise por Nacionalidade:** Qual o ranking das 20 nacionalidades com maior volume de entradas e sa√≠das no pa√≠s?
* **An√°lise por UF:** Como a movimenta√ß√£o de pessoas se distribui entre os postos de fronteira das diferentes Unidades Federativas (UF)?
* **An√°lise por Classifica√ß√£o:** Qual o volume de movimentos para cada tipo de classifica√ß√£o (ex: Visitante, Permanente, Tempor√°rio)?

### üîê Boas Pr√°ticas de Seguran√ßa
A integra√ß√£o com a AWS segue as melhores pr√°ticas de seguran√ßa. As credenciais de acesso s√£o gerenciadas fora do c√≥digo, em um arquivo `.env` (ignorado pelo Git), e carregadas dinamicamente usando a biblioteca `python-dotenv`.

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem:** Python 3
* **Bibliotecas:** `pandas`, `boto3`, `python-dotenv`
* **Ambiente:** Jupyter Notebook
* **Cloud:** Amazon Web Services (AWS) S3

## ‚öôÔ∏è Como Executar o Projeto

Siga os passos abaixo para executar o projeto.

### 1. Pr√©-requisitos
* Python e Jupyter Notebook instalados.
* Uma conta na AWS com credenciais de acesso e um bucket S3 criado.

### 2. Configure o Ambiente
* Clone este reposit√≥rio.
* Crie e ative um ambiente virtual.
* Instale as depend√™ncias: `pip install -r requirements.txt`

### 3. Configure as Credenciais da AWS
* Crie um arquivo chamado `.env` na raiz do projeto.
* Adicione suas credenciais e o nome do bucket neste formato:
    ```ini
    AWS_ACCESS_KEY_ID="SUA_CHAVE_DE_ACESSO"
    AWS_SECRET_ACCESS_KEY="SUA_CHAVE_SECRETA"
    AWS_S3_BUCKET="NOME_DO_SEU_BUCKET"
    ```

### 4. Execute os Notebooks na Ordem Correta
1.  **Execute `etl.ipynb` primeiro:** Abra e execute todas as c√©lulas para gerar os dados limpos.
2.  **Execute `analise.ipynb` em seguida:** Abra e execute todas as c√©lulas para realizar as an√°lises e enviar os resultados para o S3.

### 5. Verifique os Resultados
* Acesse o seu bucket no console da AWS S3 para confirmar que os 3 arquivos de an√°lise foram carregados com sucesso.