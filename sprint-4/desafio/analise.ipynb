{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c81cfe",
   "metadata": {},
   "source": [
    "# Etapa 2: Análise e Upload para AWS S3\n",
    "\n",
    "**Objetivo:** Carregar os dados limpos, realizar análises para responder às perguntas de negócio e fazer o upload seguro dos resultados para um bucket na AWS S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "621e84f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4a548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados limpos carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Carregamento dos Dados Limpos e Variáveis de Ambiente\n",
    "# Carrega as variáveis do arquivo .env (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_S3_BUCKET)\n",
    "load_dotenv()\n",
    "\n",
    "# Pega o nome do bucket do S3 das variáveis de ambiente\n",
    "nome_bucket_s3 = os.getenv(\"AWS_S3_BUCKET\")\n",
    "\n",
    "# Define o caminho para o arquivo de dados limpos\n",
    "caminho_dados_limpos = 'dados_processados/movimento_migratorio_limpo.csv'\n",
    "\n",
    "# Carrega os dados\n",
    "try:\n",
    "    df = pd.read_csv(caminho_dados_limpos)\n",
    "    print(\"Dados limpos carregados com sucesso!\")\n",
    "    if not nome_bucket_s3:\n",
    "        print(\"AVISO: Nome do bucket S3 não encontrado no arquivo .env.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo não encontrado. Certifique-se de executar o notebook 'etl.ipynb' primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30886eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Upload no S3\n",
    "def upload_para_s3(df_to_upload, bucket, nome_arquivo):\n",
    "    \"\"\"Função para fazer o upload de um DataFrame para o S3 de forma segura.\"\"\"\n",
    "    print(f\"Iniciando upload do arquivo '{nome_arquivo}' para o bucket '{bucket}'...\")\n",
    "    \n",
    "    try:\n",
    "        # Converte o DataFrame para um objeto CSV em memória\n",
    "        csv_buffer = StringIO()\n",
    "        df_to_upload.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "        # Cria um cliente S3. Boto3 usará as credenciais carregadas por dotenv.\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Faz o upload\n",
    "        s3_client.put_object(Bucket=bucket, Key=nome_arquivo, Body=csv_buffer.getvalue())\n",
    "        \n",
    "        print(f\"Upload de '{nome_arquivo}' concluído com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Falha no upload para o S3: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335acad3",
   "metadata": {},
   "source": [
    "### Análises e Uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5400f555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análise por Nacionalidade ---\n",
      "      NACIONALIDADE    TOTAL\n",
      "23           BRASIL  1473473\n",
      "8         ARGENTINA   207497\n",
      "38            CHILE   111028\n",
      "61   ESTADOS UNIDOS    83084\n",
      "185         URUGUAI    52479\n",
      "41         COLÔMBIA    38886\n",
      "143        PORTUGAL    34507\n",
      "66           FRANÇA    32965\n",
      "138        PARAGUAI    27607\n",
      "140            PERU    25754\n",
      "39            CHINA    24643\n",
      "92           ITÁLIA    24453\n",
      "64        FILIPINAS    23220\n",
      "189       VENEZUELA    20771\n",
      "60          ESPANHA    20058\n",
      "21          BOLÍVIA    18904\n",
      "2          ALEMANHA    18469\n",
      "146     REINO UNIDO    18180\n",
      "123          MÉXICO    18006\n",
      "195           ÍNDIA    10877\n",
      "Iniciando upload do arquivo 'analise_nacionalidade.csv' para o bucket 'desafio-sprint-4-victor-rafael'...\n",
      "Upload de 'analise_nacionalidade.csv' concluído com sucesso!\n",
      "\n",
      "--- Análise por UF de Atendimento ---\n",
      "   UF_ATENDIMENTO    TOTAL\n",
      "24             SP  1424938\n",
      "17             RJ   406188\n",
      "21             RS   104531\n",
      "16             PR    89861\n",
      "6              DF    72304\n",
      "22             SC    60252\n",
      "10             MG    48464\n",
      "4              BA    43899\n",
      "15             PE    42429\n",
      "5              CE    39930\n",
      "2              AM    19722\n",
      "13             PA    18452\n",
      "20             RR    13126\n",
      "11             MS    10784\n",
      "18             RN     7332\n",
      "3              AP     7316\n",
      "0              AC     6230\n",
      "7              ES     4026\n",
      "1              AL     2971\n",
      "19             RO     1112\n",
      "12             MT      749\n",
      "9              MA      642\n",
      "14             PB      427\n",
      "8              GO       77\n",
      "23             SE       54\n",
      "Iniciando upload do arquivo 'analise_uf.csv' para o bucket 'desafio-sprint-4-victor-rafael'...\n",
      "Upload de 'analise_uf.csv' concluído com sucesso!\n",
      "\n",
      "--- Análise por Classificação ---\n",
      "  CLASSIFICACAO    TOTAL\n",
      "0        OUTROS  1624292\n",
      "3     VISITANTE   720063\n",
      "1    PERMANENTE    74567\n",
      "2    TEMPORÁRIO     6894\n",
      "Iniciando upload do arquivo 'analise_classificacao.csv' para o bucket 'desafio-sprint-4-victor-rafael'...\n",
      "Upload de 'analise_classificacao.csv' concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Análise 1: Nacionalidade\n",
    "print(\"\\n--- Análise por Nacionalidade ---\")\n",
    "analise_nacionalidade = df.groupby('NACIONALIDADE')['TOTAL'].sum().reset_index().sort_values(by='TOTAL', ascending=False).head(20)\n",
    "print(analise_nacionalidade)\n",
    "upload_para_s3(analise_nacionalidade, nome_bucket_s3, \"analise_nacionalidade.csv\")\n",
    "\n",
    "# Análise 2: UF de Atendimento\n",
    "print(\"\\n--- Análise por UF de Atendimento ---\")\n",
    "analise_uf = df.groupby('UF_ATENDIMENTO')['TOTAL'].sum().reset_index().sort_values(by='TOTAL', ascending=False)\n",
    "print(analise_uf)\n",
    "upload_para_s3(analise_uf, nome_bucket_s3, \"analise_uf.csv\")\n",
    "\n",
    "# Análise 3: Classificação\n",
    "print(\"\\n--- Análise por Classificação ---\")\n",
    "analise_classificacao = df.groupby('CLASSIFICACAO')['TOTAL'].sum().reset_index().sort_values(by='TOTAL', ascending=False)\n",
    "print(analise_classificacao)\n",
    "upload_para_s3(analise_classificacao, nome_bucket_s3, \"analise_classificacao.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed1682",
   "metadata": {},
   "source": [
    "### Verificação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390d21d",
   "metadata": {},
   "source": [
    "## Verificação Final\n",
    "\n",
    "Uploads concluídos. Agora você pode verificar o bucket **[nome-do-seu-bucket]** no console da AWS para confirmar que os arquivos `analise_nacionalidade.csv`, `analise_uf.csv` e `analise_classificacao.csv` estão lá."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
